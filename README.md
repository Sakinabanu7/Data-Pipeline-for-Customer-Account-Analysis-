 # ğŸ’¡ Data Pipeline for Customer Account Analysis

This project demonstrates an end-to-end **data pipeline** on **Google Cloud Platform (GCP)** to process, clean, transform, and analyze customer account data. It includes data ingestion, PySpark processing on Dataproc, and upserts into **BigQuery**.

---

## ğŸ“Œ Project Objectives

- Automate customer data ingestion and transformation.
- Standardize data quality across all stages.
- Enable analytical reporting via BigQuery integration.

---

## ğŸ§± Architecture Overview

![Architecture Diagram] in the file attachment.

---

## ğŸ”„ Pipeline Steps

### ğŸŸ¤ Step 1: Raw Data Ingestion (Bronze Layer)
- ğŸ“† Used Cloud Scheduler to copy files from backend bucket to raw bucket on a schedule.

- âœ… Copied 5 CSV files using `gsutil` from backend bucket to raw bucket.

```bash
gsutil cp gs://backend-team-bucket/*.csv gs://bronze-data-pipeline-455700/
````

---

### âš™ï¸ Step 2: Cleaning & Transformation (Silver Layer)

* ğŸš€ Tool: PySpark on Dataproc
* ğŸ§¼ Cleaned 5 datasets: `accounts.csv`, `customers.csv`, `loans.csv`, `loan_payments.csv`, `transactions.csv`
* â— Schema explicitly defined (inferSchema = false)
* ğŸ§¾ Removed nulls, filtered invalid records (e.g., negative balances)
* ğŸ’¾ Output written to: `gs://silver-data-pipeline-455700/`
* ğŸ“œ Script: `clean_all_files_dataproc.py`

---

### ğŸŸ¡ Step 3: Join & Aggregation (Gold Layer)

* ğŸ‘¥ Joined `accounts` with `customers`
* â• Computed total balance per customer
* ğŸ“¤ Saved result to: `gs://gold-data-pipeline-455700/customer_account_balance/`
* ğŸ“œ Script: `join_and_aggregate.py`

---

### ğŸŸ¢ Step 4: Load to BigQuery & Upsert

* ğŸ§ª Loaded Gold data into staging table using:

```bash
bq load --source_format=PARQUET \
project_id:customer_data.refined_gold_customer_balance_temp \
gs://gold-data-pipeline-455700/customer_account_balance/*.parquet
```

* ğŸ—ï¸ Created target table using:

```bash
bash create_customer_balance_table.sh
```

* ğŸ” Upserted daily updates using:

 
bash daily_customer_balance_upsert.sh
```

--- Used Cloud Composer (Airflow) to orchestrate the full pipeline

## ğŸ§‘â€ğŸ’» My Role: Data Cleaning & Transformation Lead

I was responsible for the **data cleaning and transformation stage** using PySpark on Dataproc:

* Defined schemas manually
* Removed nulls and filtered out bad records
* Saved cleaned data to the silver bucket
* Managed the Dataproc cluster lifecycle to reduce costs

This step ensured the data quality and stability necessary for the rest of the pipeline.

---

## ğŸ—‚ï¸ Repository Structure

```
â”œâ”€â”€ clean_all_files_dataproc.py
â”œâ”€â”€ join_and_aggregate.py
â”œâ”€â”€ modify_gold_data.py
â”œâ”€â”€ create_customer_balance_table.sh
â”œâ”€â”€ daily_customer_balance_upsert.sh
â”œâ”€â”€ Data Pipeline_Arch.png
â”œâ”€â”€ Project1_pipeline_img.zip
â””â”€â”€ Customer_Data_Pipeline_Detailed.docx
```

---

## ğŸ“¸ Screenshots

All project execution screenshots are stored in:
ğŸ“ `Project1_pipeline_img.zip`

---

## ğŸ” Dataset Info

| File                | Description                   |
| ------------------- | ----------------------------- |
| `accounts.csv`      | Customer account details      |
| `customers.csv`     | Demographic data of customers |
| `loans.csv`         | Loan contract records         |
| `loan_payments.csv` | Loan payment transactions     |
| `transactions.csv`  | General transaction logs      |

---

## âœ… Outcomes

* âœ… Successfully implemented GCP-based pipeline architecture
* ğŸ§¼ Ensured data quality and standardization with PySpark
* ğŸ“Š Enabled insights via aggregated BigQuery tables

---

## ğŸ§° Tech Stack

* Google Cloud Storage (GCS)
* Google Dataproc (PySpark)
* Google BigQuery
* Shell Scripting
* Git & GitHub

---

## ğŸ Final Output

* ğŸ”¢ BigQuery Table: `customer_data.customer_account_balance`
* ğŸ§ª Staging Table: `refined_gold_customer_balance_temp`

```
## Note
â˜ï¸ Cloud Scheduler is used to automatically copy files from the backend bucket to the raw/bronze bucket on a daily or scheduled basis.

ğŸ” The entire data pipeline is orchestrated using Airflow Composer, enabling seamless end-to-end automation of all steps from ingestion to BigQuery upsert.


 
